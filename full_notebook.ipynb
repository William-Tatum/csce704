{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List of CWE IDs\n",
    "cwe_ids = [\n",
    "    14, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 134, 135, \n",
    "    170, 188, 191, 192, 194, 195, 196, 197, 242, 243, 244, 362, 364, 366, 374, 375,\n",
    "    401, 415, 416, 457, 460, 462, 463, 464, 466, 467, 468, 469, 474, 476, 478, 479,\n",
    "    480, 481, 482, 483, 484, 495, 496, 558, 560, 562, 587, 676, 685, 688, 689, 690, \n",
    "    704, 733, 762, 781, 782, 783, 785, 787, 789, 805, 806, 839, 843, 910, 911, 1325, \n",
    "    1335, 1341\n",
    "]\n",
    "\n",
    "# Function to scrape data for a specific CWE ID\n",
    "def scrape_cwe(cwe_id):\n",
    "    url = f\"https://cwe.mitre.org/data/definitions/{cwe_id}.html\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract the title\n",
    "        title = soup.title.string.strip() if soup.title else f\"CWE-{cwe_id}: Title not found\"\n",
    "        # Clean the title to remove the extra spaces and version number in parentheses\n",
    "        title = \" \".join(soup.title.string.split()).replace(\"CWE - \", \"\").split(\"(\")[0].strip()\n",
    "        \n",
    "        # Extract Description\n",
    "        description_div = soup.find('div', id=\"Description\")\n",
    "        description = description_div.find('div', class_=\"indent\").get_text(strip=True) if description_div else \"Description not found\"\n",
    "        \n",
    "        # Extract Extended Description\n",
    "        extended_description_div = soup.find('div', id=\"Extended_Description\")\n",
    "        extended_description = extended_description_div.find('div', class_=\"indent\").get_text(strip=True) if extended_description_div else \"Extended Description not found\"\n",
    "        \n",
    "        return title, description, extended_description\n",
    "    else:\n",
    "        print(f\"Failed to retrieve CWE-{cwe_id}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "# File to store the results\n",
    "output_file = \"cwe_descriptions.txt\"\n",
    "\n",
    "# Scrape and save data\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for cwe_id in cwe_ids:\n",
    "        print(f\"Proccessing CWE-{cwe_id}\")\n",
    "        title, description, extended_description = scrape_cwe(cwe_id)\n",
    "        if description and extended_description:\n",
    "            f.write(f\"Title: {title}\\n\")\n",
    "            f.write(f\"Description: {description}\\n\")\n",
    "            f.write(f\"Extended Description: {extended_description}\\n\")\n",
    "            f.write(\"\\n\" + \"-\"*80 + \"\\n\\n\")\n",
    "\n",
    "print(f\"Scraping completed. Descriptions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from openai import OpenAI\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=\"sk-svcacct-kPyRL1I2gCx0-6gW5EH5MXT3upS_euCjuQf1uaNp776BDzVWv_ixUja__XSf9Y1T3BlbkFJjfIKXBKnz-CBdOBMbxRMIQ4cpyuRQSYygrhC5AivGPeZ18ATUP3yy4XzCJtbZAA\")\n",
    "\n",
    "def create_directories():\n",
    "    \"\"\"Ensure directories for vulnerable and patched examples exist.\"\"\"\n",
    "    os.makedirs(\"vuln_data\", exist_ok=True)\n",
    "    os.makedirs(\"patched_data\", exist_ok=True)\n",
    "\n",
    "def parse_cwe_file(file_path):\n",
    "    \"\"\"Parse CWE descriptions and return structured data.\"\"\"\n",
    "    cwe_data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().strip().split(\"\\n\\n--------------------------------------------------------------------------------\\n\\n\")\n",
    "        for entry in content:\n",
    "            lines = entry.split('\\n')\n",
    "            title = lines[0].replace(\"Title: \", \"\").strip()\n",
    "            description = lines[1].replace(\"Description: \", \"\").strip()\n",
    "            extended_description = lines[2].replace(\"Extended Description: \", \"\").strip() if len(lines) > 2 else \"\"\n",
    "            cwe_data.append((title, description, extended_description))\n",
    "    return cwe_data\n",
    "\n",
    "def query_chatgpt(prompt):\n",
    "    \"\"\"Query ChatGPT API for code generation.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in identifying and fixing vulnerabilities in code. Generate code snippets as per the instructions.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    code = response.choices[0].message.content\n",
    "    code = re.sub(r\"```[a-zA-Z]*\\n|```\", \"\", code)  # Clean markdown code formatting\n",
    "    return code.strip()\n",
    "\n",
    "def generate_vulnerable_code(title, description, extended_description):\n",
    "    \"\"\"Generate a vulnerable code snippet.\"\"\"\n",
    "    prompt = (f\"Create a snippet of C code based on the following details (it must be written in C):\\n\"\n",
    "              f\"Title: {title}\\n\"\n",
    "              f\"Description: {description}\\n\"\n",
    "              f\"Extended Description: {extended_description}\\n\"\n",
    "              \"Ensure that you only print the code and nothing else as it will be going into a .txt file.\\n\"\n",
    "              \"Do not include comments and do not include markdown.\\n\")\n",
    "    return query_chatgpt(prompt)\n",
    "\n",
    "def generate_patched_code(vulnerable_code, title, description, extended_description):\n",
    "    \"\"\"Generate the patched version of the code.\"\"\"\n",
    "    prompt = (f\"Given the following vulnerable code, fix the vulnerability:\\n\\n\"\n",
    "              f\"{vulnerable_code}\\n\\n\"\n",
    "              \"Make sure the patched code adheres to secure coding practices. \"\n",
    "              \"Only output the fixed code without comments or markdown.\")\n",
    "    return query_chatgpt(prompt)\n",
    "\n",
    "def similarity(a, b):\n",
    "    \"\"\"Calculate similarity ratio between two strings.\"\"\"\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def is_unique(code, existing_codes, threshold=0.8):\n",
    "    \"\"\"Check if the code is unique based on similarity threshold.\"\"\"\n",
    "    return all(similarity(code, existing) < threshold for existing in existing_codes)\n",
    "\n",
    "def save_code_to_file(directory, title, example_num, code):\n",
    "    \"\"\"Save a code example to a file.\"\"\"\n",
    "    safe_title = re.sub(r'[^\\w\\s]', '', title).replace(' ', '_')\n",
    "    filename = f\"{directory}/{safe_title}_{example_num}.c\"\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(code)\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "def main():\n",
    "    create_directories()\n",
    "    cwe_data = parse_cwe_file('cwe_descriptions.txt')\n",
    "\n",
    "    for title, description, extended_description in cwe_data:\n",
    "        print(f\"Processing CWE: {title}\")\n",
    "\n",
    "        vulnerable_examples = []\n",
    "        patched_examples = []\n",
    "\n",
    "        for example_num in range(10):  # Generate 10 examples per CWE\n",
    "            # Generate a unique vulnerable example\n",
    "            while True:\n",
    "                vulnerable_code = generate_vulnerable_code(title, description, extended_description)\n",
    "                if is_unique(vulnerable_code, vulnerable_examples):\n",
    "                    break\n",
    "            vulnerable_examples.append(vulnerable_code)\n",
    "\n",
    "            # Generate the corresponding patched example\n",
    "            patched_code = generate_patched_code(vulnerable_code, title, description, extended_description)\n",
    "            patched_examples.append(patched_code)\n",
    "\n",
    "            # Save each example to separate files\n",
    "            save_code_to_file(\"vuln_data\", title, example_num, vulnerable_code)\n",
    "            save_code_to_file(\"patched_data\", title, example_num, patched_code)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import networkx as nx\n",
    "from clang.cindex import Index, Config\n",
    "\n",
    "# Configure libclang path\n",
    "Config.set_library_file('.venv/lib/python3.12/site-packages/clang/native/libclang.so')\n",
    "\n",
    "# Define token mappings\n",
    "TOKEN_TYPE_MAP = {\n",
    "    \"KEYWORD\": 1,\n",
    "    \"SYMBOL\": 2,\n",
    "    \"NUMBER\": 3,\n",
    "    \"FUNCTION\": 4,\n",
    "    \"IDENTIFIER\": 5,\n",
    "    \"UNK\": 6\n",
    "}\n",
    "\n",
    "KEYWORDS = {'int', 'float', 'return', 'if', 'else', 'for', 'while', 'void', 'size_t', 'char', 'const', 'volatile', 'unsigned'}\n",
    "SYMBOLS = {'(', ')', '{', '}', '[', ']', ';', ',', '->', '++', '--', '=', '==', '!=', '<', '>', '<=', '>=', '+', '-', '*', '/', '%', '&', '|', '^', '~', '!', '<<', '>>'}\n",
    "FUNCTIONS = {'strcpy', 'memcpy', 'malloc', 'free', 'printf', 'fgets', 'strlen', 'strncpy', 'explicit_bzero'}\n",
    "UNK = TOKEN_TYPE_MAP[\"UNK\"]\n",
    "\n",
    "NUMBER_PATTERN = r'\\b\\d+\\b'\n",
    "IDENTIFIER_PATTERN = r'\\b[A-Za-z_][A-Za-z0-9_]*\\b'\n",
    "\n",
    "def tokenize_ast(code):\n",
    "    tokens = []\n",
    "    try:\n",
    "        with open(\"temp.c\", \"w\") as temp_file:\n",
    "            temp_file.write(code)\n",
    "        index = Index.create()\n",
    "        tu = index.parse(\"temp.c\")\n",
    "        def visit_node(node):\n",
    "            if node.kind is not None:\n",
    "                tokens.append(node.kind.name)\n",
    "            for child in node.get_children():\n",
    "                visit_node(child)\n",
    "        visit_node(tu.cursor)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing AST: {e}\")\n",
    "        tokens.append(\"UNK\")\n",
    "    return tokens\n",
    "\n",
    "def syntax_aware_tokenize(code):\n",
    "    tokens = []\n",
    "    words = re.findall(r'\\w+|\\S', code)\n",
    "    for word in words:\n",
    "        if word in KEYWORDS:\n",
    "            tokens.append(\"KEYWORD\")\n",
    "        elif word in SYMBOLS:\n",
    "            tokens.append(\"SYMBOL\")\n",
    "        elif re.fullmatch(NUMBER_PATTERN, word):\n",
    "            tokens.append(\"NUMBER\")\n",
    "        elif word in FUNCTIONS:\n",
    "            tokens.append(\"FUNCTION\")\n",
    "        elif re.fullmatch(IDENTIFIER_PATTERN, word):\n",
    "            tokens.append(\"IDENTIFIER\")\n",
    "        else:\n",
    "            tokens.append(\"UNK\")\n",
    "    return tokens\n",
    "\n",
    "def generate_cfg(code):\n",
    "    \"\"\"\n",
    "    Generate a control flow graph (CFG) using libclang.\n",
    "    Args:\n",
    "        code (str): C code to analyze.\n",
    "    Returns:\n",
    "        list: List of edges representing the CFG.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    try:\n",
    "        # Write the code to a temporary file for libclang to parse\n",
    "        with open(\"temp.c\", \"w\") as temp_file:\n",
    "            temp_file.write(code)\n",
    "\n",
    "        # Parse the code with libclang\n",
    "        index = Index.create()\n",
    "        tu = index.parse(\"temp.c\")\n",
    "\n",
    "        # Helper function to add nodes and edges\n",
    "        def visit_node(node, parent=None):\n",
    "            node_id = node.spelling or node.displayname or node.kind.name\n",
    "            if not node_id:\n",
    "                node_id = f\"Node_{node.hash}\"\n",
    "            G.add_node(node_id, kind=node.kind.name)\n",
    "\n",
    "            if parent is not None:\n",
    "                G.add_edge(parent, node_id)\n",
    "\n",
    "            # Recursively visit children\n",
    "            for child in node.get_children():\n",
    "                visit_node(child, node_id)\n",
    "\n",
    "        # Start visiting from the root cursor\n",
    "        visit_node(tu.cursor)\n",
    "\n",
    "        # Return edges as a list for storage\n",
    "        return list(G.edges)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating CFG: {e}\")\n",
    "        return [(\"UNK\", \"UNK\")]\n",
    "\n",
    "\n",
    "def tokenize_code(code, method=\"basic\"):\n",
    "    if method == \"ast\":\n",
    "        return tokenize_ast(code)\n",
    "    elif method == \"syntax\":\n",
    "        return syntax_aware_tokenize(code)\n",
    "    elif method == \"cfg\":\n",
    "        return generate_cfg(code)\n",
    "    else:\n",
    "        tokens = []\n",
    "        words = re.findall(r'\\w+|\\S', code)\n",
    "        for word in words:\n",
    "            if word in KEYWORDS:\n",
    "                tokens.append(TOKEN_TYPE_MAP[\"KEYWORD\"])\n",
    "            elif word in SYMBOLS:\n",
    "                tokens.append(TOKEN_TYPE_MAP[\"SYMBOL\"])\n",
    "            elif re.fullmatch(NUMBER_PATTERN, word):\n",
    "                tokens.append(TOKEN_TYPE_MAP[\"NUMBER\"])\n",
    "            elif word in FUNCTIONS:\n",
    "                tokens.append(TOKEN_TYPE_MAP[\"FUNCTION\"])\n",
    "            elif re.fullmatch(IDENTIFIER_PATTERN, word):\n",
    "                tokens.append(TOKEN_TYPE_MAP[\"IDENTIFIER\"])\n",
    "            else:\n",
    "                tokens.append(UNK)\n",
    "        return tokens\n",
    "\n",
    "def save_as_json(output_file, tokens, label):\n",
    "    data = {\"tokens\": tokens, \"label\": label}\n",
    "    with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file)\n",
    "    print(f\"Saved JSON: {output_file}\")\n",
    "\n",
    "def tokenize_file(input_file, output_file_base, label, method=\"basic\"):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        code = infile.read()\n",
    "    tokens = tokenize_code(code, method=method)\n",
    "    save_as_json(f\"{output_file_base}_{method}.json\", tokens, label)\n",
    "\n",
    "def process_directory(input_dir, output_dir, label, method=\"basic\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.c'):\n",
    "                input_path = os.path.join(root, file)\n",
    "                base_filename = file.replace('.c', '')\n",
    "                output_file_base = os.path.join(output_dir, base_filename)\n",
    "                tokenize_file(input_path, output_file_base, label, method=method)\n",
    "\n",
    "def process_test_directory(test_dir, vuln_output_dir, patched_output_dir, method=\"basic\"):\n",
    "    os.makedirs(vuln_output_dir, exist_ok=True)\n",
    "    os.makedirs(patched_output_dir, exist_ok=True)\n",
    "    for root, _, files in os.walk(test_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.c'):\n",
    "                input_path = os.path.join(root, file)\n",
    "                base_filename = file.replace('.c', '')\n",
    "                if \"CLEAN\" in file.upper():\n",
    "                    output_file_base = os.path.join(patched_output_dir, base_filename)\n",
    "                    label = 0\n",
    "                else:\n",
    "                    output_file_base = os.path.join(vuln_output_dir, base_filename)\n",
    "                    label = 1\n",
    "                tokenize_file(input_path, output_file_base, label, method=method)\n",
    "\n",
    "def main():\n",
    "    vuln_input_dir = \"vuln_data\"\n",
    "    patched_input_dir = \"patched_data\"\n",
    "    test_dir = \"TestCode\"\n",
    "    methods = [\"basic\", \"ast\", \"syntax\", \"cfg\"]\n",
    "\n",
    "    for method in methods:\n",
    "        print(f\"Tokenizing with method: {method}...\")\n",
    "        process_directory(vuln_input_dir, f\"vuln_train_{method}\", label=1, method=method)\n",
    "        process_directory(patched_input_dir, f\"patched_train_{method}\", label=0, method=method)\n",
    "        process_test_directory(test_dir, f\"vuln_test_{method}\", f\"patched_test_{method}\", method=method)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to create the custom environment for DQN\n",
    "def create_env(X_train_encoded, y_train_full):\n",
    "    \"\"\"\n",
    "    Create a custom environment for training DQN.\n",
    "\n",
    "    Args:\n",
    "        X_train_encoded: The tokenized and encoded training sequences.\n",
    "        y_train_full: The labels for the training data.\n",
    "\n",
    "    Returns:\n",
    "        env: A gym environment for DQN.\n",
    "    \"\"\"\n",
    "    class CustomEnv(gym.Env):\n",
    "        def __init__(self, X_train_encoded, y_train_full):\n",
    "            super(CustomEnv, self).__init__()\n",
    "            self.X = X_train_encoded\n",
    "            self.y = y_train_full\n",
    "            self.current_idx = 0\n",
    "            self.action_space = gym.spaces.Discrete(2)  # Assume binary classification (patched or vulnerable)\n",
    "            self.observation_space = gym.spaces.Box(low=0, high=1, shape=(X_train_encoded.shape[1],), dtype=np.float32)\n",
    "        \n",
    "        def reset(self):\n",
    "            self.current_idx = 0\n",
    "            return self.X[self.current_idx]\n",
    "        \n",
    "        def step(self, action):\n",
    "            reward = 1 if action == self.y[self.current_idx] else -1\n",
    "            self.current_idx += 1\n",
    "            done = self.current_idx >= len(self.X)\n",
    "            next_state = self.X[self.current_idx] if not done else np.zeros_like(self.X[0])\n",
    "            return next_state, reward, done, {}\n",
    "\n",
    "        def render(self, mode='human'):\n",
    "            pass\n",
    "\n",
    "    env = CustomEnv(X_train_encoded, y_train_full)\n",
    "    return env\n",
    "\n",
    "\n",
    "# Function to train and evaluate the DQN model\n",
    "def train_and_eval_dqn(model, X_train_encoded, y_train_full, X_test_encoded, y_test):\n",
    "    \"\"\"\n",
    "    Train the DQN model and evaluate its performance.\n",
    "\n",
    "    Args:\n",
    "        model: The DQN model.\n",
    "        X_train_encoded: The tokenized and encoded training sequences.\n",
    "        y_train_full: The labels for the training data.\n",
    "        X_test_encoded: The encoded test data.\n",
    "        y_test: The labels for the test data.\n",
    "\n",
    "    Returns:\n",
    "        None: Outputs evaluation metrics and saves the model.\n",
    "    \"\"\"\n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=100000)  # Change timesteps if necessary\n",
    "    \n",
    "    # Predict on the training set\n",
    "    train_preds = model.predict(X_train_encoded, deterministic=True)[0]\n",
    "    train_accuracy = accuracy_score(y_train_full, train_preds)\n",
    "\n",
    "    # Predict on the test set\n",
    "    test_preds = model.predict(X_test_encoded, deterministic=True)[0]\n",
    "    test_accuracy = accuracy_score(y_test, test_preds)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "    \n",
    "    return train_accuracy, test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of CNN jupyter notebook\n",
    "\n",
    "### Loading Preprocessed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.api.models import Sequential, Model\n",
    "from keras.api.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Input, Concatenate, LSTM, Bidirectional, Dot, Flatten, Layer, Activation, MultiHeadAttention, LayerNormalization\n",
    "from keras.api.optimizers import Adam, SGD\n",
    "from keras.api.optimizers.schedules import ExponentialDecay, CosineDecay\n",
    "from keras.api.callbacks import LearningRateScheduler, Callback, ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve, average_precision_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from directories\n",
    "def load_data_from_directory(directory):\n",
    "    data = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                with open(os.path.join(root, file), 'r') as f:\n",
    "                    entry = json.load(f)\n",
    "                    data.append((entry[\"tokens\"], entry[\"label\"]))\n",
    "    return data\n",
    "\n",
    "# Helper function to load data for a given tokenization method\n",
    "def load_data_for_method(method):\n",
    "    train_data = load_data_from_directory(f\"vuln_train_{method}\") + load_data_from_directory(f\"patched_train_{method}\")\n",
    "    test_data = load_data_from_directory(f\"vuln_test_{method}\") + load_data_from_directory(f\"patched_test_{method}\")\n",
    "    \n",
    "    X_train_full = [entry[0] for entry in train_data]\n",
    "    y_train_full = [entry[1] for entry in train_data]\n",
    "    X_test = [entry[0] for entry in test_data]\n",
    "    y_test = [entry[1] for entry in test_data]\n",
    "    \n",
    "    return X_train_full, y_train_full, X_test, y_test\n",
    "\n",
    "# Set tokenization methods\n",
    "tokenization_methods = [\"basic\", \"ast\", \"normalize\", \"syntax\", \"cfg\"]\n",
    "\n",
    "# Build vocabulary from training data\n",
    "def build_vocab(X_train_full):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary from the training data.\n",
    "\n",
    "    Args:\n",
    "        X_train_full (list of lists): Tokenized training data.\n",
    "\n",
    "    Returns:\n",
    "        dict: Vocabulary mapping tokens to unique IDs.\n",
    "    \"\"\"\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    current_id = 2\n",
    "\n",
    "    for sequence in X_train_full:\n",
    "        for token in (token for sublist in sequence for token in sublist) if isinstance(sequence[0], list) else sequence:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = current_id\n",
    "                current_id += 1\n",
    "    return vocab\n",
    "\n",
    "# Normalize nested sequences during token loading\n",
    "def normalize_sequences(sequences):\n",
    "    return [\n",
    "        [token for sublist in sequence for token in sublist] if sequence and isinstance(sequence[0], list) else sequence or [\"<UNK>\"]\n",
    "        for sequence in sequences\n",
    "    ]\n",
    "\n",
    "# Encode sequences using vocabulary\n",
    "def encode_sequences(sequences, vocab, max_length):\n",
    "    unk_id = vocab[\"<UNK>\"]\n",
    "    encoded = []\n",
    "    for sequence in sequences:\n",
    "        encoded_sequence = [vocab.get(token, unk_id) for token in sequence]\n",
    "        if len(encoded_sequence) > max_length:\n",
    "            encoded_sequence = encoded_sequence[:max_length]\n",
    "        else:\n",
    "            encoded_sequence += [vocab[\"<PAD>\"]] * (max_length - len(encoded_sequence))\n",
    "        encoded.append(encoded_sequence)\n",
    "    return np.array(encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding Layer\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_length = tf.shape(inputs)[1]\n",
    "        position = tf.range(seq_length, dtype=tf.float32)[:, tf.newaxis]\n",
    "        div_term = tf.exp(tf.range(0, self.embedding_dim, 2, dtype=tf.float32) * -(np.log(10000.0) / self.embedding_dim))\n",
    "        positional_encoding = tf.concat([tf.sin(position * div_term), tf.cos(position * div_term)], axis=1)\n",
    "        positional_encoding = positional_encoding[tf.newaxis, ...]\n",
    "        return inputs + positional_encoding[:, :seq_length, :]\n",
    "\n",
    "# Transformer Encoder Layer\n",
    "def transformer_encoder(inputs, num_heads, key_dim, ff_dim, dropout_rate):\n",
    "    attention = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(inputs, inputs)\n",
    "    attention = Dropout(dropout_rate)(attention)\n",
    "    attention = LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    ff_output = Dense(ff_dim, activation=\"relu\")(attention)\n",
    "    ff_output = Dense(inputs.shape[-1])(ff_output)\n",
    "    ff_output = Dropout(dropout_rate)(ff_output)\n",
    "    return LayerNormalization(epsilon=1e-6)(attention + ff_output)\n",
    "\n",
    "# Build the CNN-BiLSTM Model\n",
    "def build_cnn_model(vocab_size, embedding_dim=256, num_filters=256, kernel_sizes=[3, 5, 7],\n",
    "                    lstm_units=256, dense_units=256, dropout_rate=0.5, l2_lambda=0.01, num_heads=4):\n",
    "    input_layer = Input(shape=(None,), dtype=\"int32\")\n",
    "    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)(input_layer)\n",
    "    \n",
    "    positional_encoding = PositionalEncoding(embedding_dim=embedding_dim)(embedding)\n",
    "    transformer = transformer_encoder(positional_encoding, num_heads=num_heads, key_dim=embedding_dim, ff_dim=512, dropout_rate=0.1)\n",
    "    \n",
    "    conv_layers = []\n",
    "    for kernel_size in kernel_sizes:\n",
    "        conv = Conv1D(filters=num_filters, kernel_size=kernel_size, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_lambda))(transformer)\n",
    "        pooled = GlobalMaxPooling1D()(conv)\n",
    "        conv_layers.append(pooled)\n",
    "    \n",
    "    conv_features = Concatenate()(conv_layers)\n",
    "    lstm = Bidirectional(LSTM(lstm_units, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))(transformer)\n",
    "    \n",
    "    attention = Dense(1, activation=\"tanh\")(lstm)\n",
    "    attention = Flatten()(attention)\n",
    "    attention_weights = tf.keras.layers.Activation(\"softmax\")(attention)\n",
    "    attention_output = tf.keras.layers.Dot(axes=1)([attention_weights, lstm])\n",
    "    \n",
    "    combined = Concatenate()([conv_features, attention_output])\n",
    "    dense = Dense(dense_units, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_lambda))(combined)\n",
    "    dropout = Dropout(dropout_rate)(dense)\n",
    "    output = Dense(1, activation=\"sigmoid\")(dropout)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss=\"binary_crossentropy\", metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the exponential decay learning rate schedule\n",
    "# def exponential_decay_schedule():\n",
    "#     return ExponentialDecay(\n",
    "#         initial_learning_rate=1e-3,\n",
    "#         decay_steps=1000,\n",
    "#         decay_rate=0.96,\n",
    "#         staircase=True\n",
    "#     )\n",
    "\n",
    "# # Define the warmup with cosine decay learning rate schedule\n",
    "# def warmup_cosine_decay_schedule(epoch):\n",
    "#     if epoch < 5:  # Warmup phase\n",
    "#         return float(1e-5 + (epoch / 5) * (1e-3 - 1e-5))  # Python float\n",
    "#     else:  # Cosine decay phase\n",
    "#         decay = CosineDecay(initial_learning_rate=1e-3, decay_steps=15)\n",
    "#         return float(decay(epoch - 5))  # Python float\n",
    "\n",
    "# # Cyclical Learning Rate\n",
    "# def cyclical_learning_rate_schedule(base_lr=1e-5, max_lr=1e-3, step_size=2000):\n",
    "#     def lr_schedule(batch):\n",
    "#         cycle = math.floor(1 + batch / (2 * step_size))\n",
    "#         x = abs(batch / step_size - 2 * cycle + 1)\n",
    "#         lr = base_lr + (max_lr - base_lr) * max(0, (1 - x))\n",
    "#         return float(lr)  # Ensure the return value is a Python float\n",
    "#     return lr_schedule\n",
    "\n",
    "# # Train with ReduceLROnPlateau\n",
    "# def train_with_reduce_lr():\n",
    "#     reduce_lr = ReduceLROnPlateau(\n",
    "#         monitor='val_loss',\n",
    "#         factor=0.5,\n",
    "#         patience=3,\n",
    "#         min_lr=1e-6\n",
    "#     )\n",
    "#     return None, [reduce_lr]\n",
    "\n",
    "# # Train with Exponential Decay\n",
    "# def train_with_exponential_decay():\n",
    "#     lr_schedule = exponential_decay_schedule()\n",
    "#     optimizer = SGD(learning_rate=lr_schedule, momentum=0.9)\n",
    "#     return optimizer, []\n",
    "\n",
    "# # Train with Warmup + Cosine Decay\n",
    "# def train_with_warmup_cosine_decay():\n",
    "#     lr_scheduler = LearningRateScheduler(warmup_cosine_decay_schedule)\n",
    "#     return None, [lr_scheduler]\n",
    "\n",
    "# # Train with Cyclical Learning Rate\n",
    "# def train_with_cyclical_lr(base_lr=1e-5, max_lr=1e-3, step_size=2000):\n",
    "#     lr_schedule = cyclical_learning_rate_schedule(base_lr, max_lr, step_size)\n",
    "#     lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "#     return None, [lr_scheduler]\n",
    "\n",
    "# # Function to train the model with a specific strategy\n",
    "# def train_with_strategy(strategy_name, optimizer=None, callbacks=[]):\n",
    "#     print(f\"Training with {strategy_name}...\")\n",
    "\n",
    "#     # Use default SGD optimizer if not provided\n",
    "#     if not optimizer:\n",
    "#         optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "#     # Build and compile the model\n",
    "#     model = build_cnn_model(vocab_size=vocab_size, embedding_dim=128)\n",
    "#     model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "#     # Train the model\n",
    "#     history = model.fit(\n",
    "#         X_train, y_train,\n",
    "#         validation_data=(X_val, y_val),\n",
    "#         epochs=10,\n",
    "#         batch_size=32,\n",
    "#         callbacks=callbacks,\n",
    "#         verbose=1\n",
    "#     )\n",
    "#     return history\n",
    "\n",
    "# # Execute strategies\n",
    "# strategies = {\n",
    "#     \"Cyclical Learning Rate\": train_with_cyclical_lr(),\n",
    "#     \"Exponential Decay\": train_with_exponential_decay(),\n",
    "#     \"ReduceLROnPlateau\": train_with_reduce_lr(),\n",
    "#     \"Warmup + Cosine Decay\": train_with_warmup_cosine_decay()\n",
    "# }\n",
    "\n",
    "# history_records = {}\n",
    "\n",
    "# for strategy_name, (optimizer, callbacks) in strategies.items():\n",
    "#     history = train_with_strategy(strategy_name, optimizer=optimizer, callbacks=callbacks)\n",
    "#     history_records[strategy_name] = history\n",
    "\n",
    "# # Plot validation accuracy\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for strategy_name, history in history_records.items():\n",
    "#     plt.plot(history.history['val_accuracy'], label=strategy_name)\n",
    "# plt.title(\"Validation Accuracy by Strategy\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Validation Accuracy\")\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training CNN Using Best Method (so far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in tokenization_methods:\n",
    "    print(f\"Processing tokenization method: {method}\")\n",
    "    \n",
    "    # Load data\n",
    "    X_train_full, y_train_full, X_test, y_test = load_data_for_method(method)\n",
    "    \n",
    "    X_train_full = normalize_sequences(X_train_full)\n",
    "    X_test = normalize_sequences(X_test)\n",
    "\n",
    "    # Build vocabulary\n",
    "    vocab = build_vocab(X_train_full)\n",
    "    vocab_size = len(vocab)\n",
    "    max_length = 512  # Set a fixed maximum sequence length\n",
    "    \n",
    "    # Encode sequences\n",
    "    X_train_full_encoded = encode_sequences(X_train_full, vocab, max_length)\n",
    "    X_test_encoded = encode_sequences(X_test, vocab, max_length)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full_encoded, \n",
    "        np.array(y_train_full), \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    # Build model\n",
    "    cnn_model = build_cnn_model(vocab_size=vocab_size)\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    # Train model\n",
    "    history = cnn_model.fit(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        validation_data=(X_val, y_val), \n",
    "        epochs=2,  # Use more epochs for better results\n",
    "        batch_size=32, \n",
    "        callbacks=[early_stopping, reduce_lr_callback], \n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate model\n",
    "    y_pred_prob = cnn_model.predict(X_test_encoded, verbose=1)\n",
    "    test_results = cnn_model.evaluate(X_test_encoded, y_test, verbose=1)\n",
    "    \n",
    "    # Compute ROC and PRC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "    prc_auc = average_precision_score(y_test, y_pred_prob)\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr, color='b', lw=2, label=f\"AUC = {roc_auc:.4f}\")\n",
    "    plt.plot([0, 1], [0, 1], color='r', linestyle='--', label=\"Random Baseline\")\n",
    "    plt.title(f\"{method.upper()} - ROC Curve\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot PRC Curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(recall, precision, color='b', lw=2, label=f\"AP = {prc_auc:.4f}\")\n",
    "    plt.axhline(y=0.5, color='r', linestyle='--', label=\"Random Baseline\")\n",
    "    plt.title(f\"{method.upper()} - Precision-Recall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 704dqn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import AdamW\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Directory for saving metrics and charts\n",
    "output_dir = \"dqn_metrics_output3\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load data from directories\n",
    "def load_data_from_directory(directory):\n",
    "    data = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                with open(os.path.join(root, file), 'r') as f:\n",
    "                    entry = json.load(f)\n",
    "                    data.append((entry[\"tokens\"], entry[\"label\"]))\n",
    "    return data\n",
    "\n",
    "# Helper function to load data for a given tokenization method\n",
    "def load_data_for_method(method):\n",
    "    train_data = load_data_from_directory(f\"vuln_train_{method}\") + load_data_from_directory(f\"patched_train_{method}\")\n",
    "    test_data = load_data_from_directory(f\"vuln_test_{method}\") + load_data_from_directory(f\"patched_test_{method}\")\n",
    "    X_train_full = [entry[0] for entry in train_data]\n",
    "    y_train_full = [entry[1] for entry in train_data]\n",
    "    X_test = [entry[0] for entry in test_data]\n",
    "    y_test = [entry[1] for entry in test_data]\n",
    "    return X_train_full, y_train_full, X_test, y_test\n",
    "\n",
    "# Normalize nested sequences and build vocabulary\n",
    "def normalize_sequences(sequences):\n",
    "    return [\n",
    "        [token for sublist in sequence for token in sublist] if sequence and isinstance(sequence[0], list) else sequence or [\"<UNK>\"]\n",
    "        for sequence in sequences\n",
    "    ]\n",
    "\n",
    "def build_vocab(X_train_full):\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    current_id = 2\n",
    "    for sequence in X_train_full:\n",
    "        for token in (token for sublist in sequence for token in sublist) if isinstance(sequence[0], list) else sequence:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = current_id\n",
    "                current_id += 1\n",
    "    return vocab\n",
    "\n",
    "def encode_sequences(sequences, vocab, max_length):\n",
    "    unk_id = vocab[\"<UNK>\"]\n",
    "    encoded = []\n",
    "    for sequence in sequences:\n",
    "        encoded_sequence = [vocab.get(token, unk_id) for token in sequence]\n",
    "        if len(encoded_sequence) > max_length:\n",
    "            encoded_sequence = encoded_sequence[:max_length]\n",
    "        else:\n",
    "            encoded_sequence += [vocab[\"<PAD>\"]] * (max_length - len(encoded_sequence))\n",
    "        encoded.append(encoded_sequence)\n",
    "    return np.array(encoded)\n",
    "\n",
    "# Double DQN\n",
    "class QFunction(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes):\n",
    "        super().__init__()\n",
    "        sizes = [obs_dim] + hidden_sizes + [act_dim]\n",
    "        self.layers = nn.ModuleList([nn.Linear(sizes[i], sizes[i + 1]) for i in range(len(sizes) - 1)])\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = obs\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        return self.layers[-1](x)\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, obs_dim, act_dim, options):\n",
    "        self.model = QFunction(obs_dim, act_dim, options['hidden_sizes'])\n",
    "        self.target_model = deepcopy(self.model)\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=options['lr'])\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "        self.memory = deque(maxlen=options['memory_size'])\n",
    "        self.gamma = options['gamma']\n",
    "        self.batch_size = options['batch_size']\n",
    "        self.target_update_freq = options['target_update_freq']\n",
    "        self.n_steps = 0\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Double DQN: Get the Q-values for next state from the model, then use target network for target Q-values\n",
    "        q_values = self.model(states).gather(1, actions.view(-1, 1)).squeeze()\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.model(next_states).max(1)[1]  # Get the best action from model\n",
    "            target_q_values = self.target_model(next_states).gather(1, next_q_values.view(-1, 1)).squeeze()\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * target_q_values\n",
    "\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.n_steps % self.target_update_freq == 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.n_steps += 1\n",
    "\n",
    "    def evaluate(self, X_test, y_test, vocab, max_length):\n",
    "        X_test_encoded = encode_sequences(normalize_sequences(X_test), vocab, max_length)\n",
    "        y_probs = []\n",
    "        y_pred = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for state in torch.tensor(X_test_encoded, dtype=torch.float32):\n",
    "                q_values = self.model(state.unsqueeze(0))\n",
    "                probs = torch.softmax(q_values, dim=-1).squeeze()\n",
    "                y_probs.append(probs[1].item())\n",
    "                y_pred.append(probs.argmax().item())\n",
    "        y_probs = np.array(y_probs)\n",
    "        y_pred = np.array(y_pred)\n",
    "\n",
    "        # Save metrics and charts\n",
    "        report = classification_report(y_test, y_pred, target_names=[\"Non-vulnerable\", \"Vulnerable\"])\n",
    "        with open(os.path.join(output_dir, \"classification_report.txt\"), \"w\") as f:\n",
    "            f.write(report)\n",
    "\n",
    "        # ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "        plt.plot([0, 1], [0, 1], color=\"navy\", linestyle=\"--\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"Receiver Operating Characteristic\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(os.path.join(output_dir, \"roc_curve.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # PRC curve\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
    "        avg_precision = average_precision_score(y_test, y_probs)\n",
    "        plt.figure()\n",
    "        plt.step(recall, precision, where=\"post\", label=f\"Avg Precision = {avg_precision:.2f}\")\n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.title(\"Precision-Recall Curve\")\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.savefig(os.path.join(output_dir, \"prc_curve.png\"))\n",
    "        plt.close()\n",
    "\n",
    "# Parameters and training loop\n",
    "options = {'hidden_sizes': [128, 64], 'lr': 0.001, 'gamma': 0.99, 'memory_size': 5000, 'batch_size': 32, 'target_update_freq': 100}\n",
    "method = \"basic\"\n",
    "X_train, y_train, X_test, y_test = load_data_for_method(method)\n",
    "vocab = build_vocab(normalize_sequences(X_train))\n",
    "max_length = 100\n",
    "X_train_encoded = encode_sequences(normalize_sequences(X_train), vocab, max_length)\n",
    "X_test_encoded = encode_sequences(normalize_sequences(X_test), vocab, max_length)\n",
    "obs_dim = X_train_encoded.shape[1]\n",
    "act_dim = 2\n",
    "dqn = DQN(obs_dim, act_dim, options)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for i in range(len(X_train_encoded)):\n",
    "        state = X_train_encoded[i]\n",
    "        action = y_train[i]\n",
    "        reward = 1 if action == 1 else -1\n",
    "        next_state = state\n",
    "        done = True\n",
    "        dqn.memorize(state, action, reward, next_state, done)\n",
    "        dqn.train_step()\n",
    "\n",
    "dqn.evaluate(X_test, y_test, vocab, max_length)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time: {end-start}\")\n",
    "\n",
    "# Orginal \n",
    "# 10 Epochs = 19.26 (dqn_metrics_output)\n",
    "# 100 Epochs = 240.5 (dqn_metrics_output2)\n",
    "\n",
    "# Modified w/ DDQN\n",
    "# 10 Epochs = 20.2 (dqn_metrics_output3)\n",
    "# 100 Epochs = 814 (dqn_metrics_output4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "704dqnorg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load preprocessed data\n",
    "with open('processed_data.json', 'r') as infile:\n",
    "    data = json.load(infile)\n",
    "\n",
    "X = np.array(data['sequences'])  # Tokenized and padded sequences\n",
    "y = np.array(data['labels'])     # Corresponding labels (1 for vulnerable)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "class QFunction(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes):\n",
    "        super().__init__()\n",
    "        sizes = [obs_dim] + hidden_sizes + [act_dim]\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = torch.cat([obs], dim=-1)\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = F.relu(self.layers[i](x))\n",
    "        return self.layers[-1](x).squeeze(dim=-1)\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, obs_dim, act_dim, options):\n",
    "        self.model = QFunction(obs_dim, act_dim, options['layers'])\n",
    "        self.target_model = deepcopy(self.model)\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=options['alpha'], amsgrad=True)\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "        # Freeze target network parameters\n",
    "        for p in self.target_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.replay_memory = deque(maxlen=options['replay_memory_size'])\n",
    "        self.options = options\n",
    "        self.n_steps = 0\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.replay_memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def compute_target_values(self, next_states, rewards, dones):\n",
    "        return rewards + self.options['gamma'] * torch.max(self.target_model(next_states), dim=-1)[0] * (1 - dones)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.replay_memory) > self.options['batch_size']:\n",
    "            minibatch = random.sample(self.replay_memory, self.options['batch_size'])\n",
    "            states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "            states = torch.as_tensor(states, dtype=torch.float32)\n",
    "            actions = torch.as_tensor(actions, dtype=torch.long)\n",
    "            rewards = torch.as_tensor(rewards, dtype=torch.float32)\n",
    "            next_states = torch.as_tensor(next_states, dtype=torch.float32)\n",
    "            dones = torch.as_tensor(dones, dtype=torch.float32)\n",
    "\n",
    "            current_q = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(-1)\n",
    "            with torch.no_grad():\n",
    "                target_q = self.compute_target_values(next_states, rewards, dones)\n",
    "\n",
    "            loss = self.loss_fn(current_q, target_q)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(self.model.parameters(), 100)\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        for epoch in range(self.options['epochs']):\n",
    "            for i in range(len(X_train)):\n",
    "                state = X_train[i]\n",
    "                action = y_train[i]\n",
    "                reward = 1 if action == 1 else -1  # Reward based on label\n",
    "                next_state = state  # No transition in supervised setup\n",
    "                done = True\n",
    "\n",
    "                self.memorize(state, action, reward, next_state, done)\n",
    "                self.replay()\n",
    "\n",
    "                if self.n_steps % self.options['update_target_estimator_every'] == 0:\n",
    "                    self.update_target_model()\n",
    "\n",
    "                self.n_steps += 1\n",
    "\n",
    "            print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "    def evaluate(self, X_val, y_val):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            for i in range(len(X_val)):\n",
    "                state = torch.as_tensor(X_val[i], dtype=torch.float32).unsqueeze(0)\n",
    "                q_values = self.model(state)\n",
    "                predicted_action = torch.argmax(q_values).item()\n",
    "                predictions.append(predicted_action)\n",
    "\n",
    "        accuracy = accuracy_score(y_val, predictions)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        self.model.train()\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"DQN\"\n",
    "\n",
    "\n",
    "# Define options for the DQN\n",
    "options = {\n",
    "    'layers': [128, 64],\n",
    "    'alpha': 0.001,\n",
    "    'gamma': 0.99,\n",
    "    'replay_memory_size': 10000,\n",
    "    'batch_size': 32,\n",
    "    'update_target_estimator_every': 100,\n",
    "    'epochs': 10\n",
    "}\n",
    "\n",
    "# Initialize and train the model\n",
    "obs_dim = X_train.shape[1]\n",
    "act_dim = 2  # Binary classification (0 or 1)\n",
    "dqn = DQN(obs_dim, act_dim, options)\n",
    "\n",
    "dqn.train(X_train, y_train)\n",
    "dqn.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new704dqn.py - Best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, auc\n",
    "from stable_baselines3 import DQN\n",
    "from train import train_and_eval_dqn, create_env\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data from directories\n",
    "def load_data_from_directory(directory):\n",
    "    data = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                with open(os.path.join(root, file), 'r') as f:\n",
    "                    entry = json.load(f)\n",
    "                    data.append((entry[\"tokens\"], entry[\"label\"]))\n",
    "    return data\n",
    "\n",
    "# Helper function to load data for a given tokenization method\n",
    "def load_data_for_method(method):\n",
    "    train_data = load_data_from_directory(f\"vuln_train_{method}\") + load_data_from_directory(f\"patched_train_{method}\")\n",
    "    test_data = load_data_from_directory(f\"vuln_test_{method}\") + load_data_from_directory(f\"patched_test_{method}\")\n",
    "    \n",
    "    X_train_full = [entry[0] for entry in train_data]\n",
    "    y_train_full = [entry[1] for entry in train_data]\n",
    "    X_test = [entry[0] for entry in test_data]\n",
    "    y_test = [entry[1] for entry in test_data]\n",
    "    \n",
    "    return X_train_full, y_train_full, X_test, y_test\n",
    "\n",
    "# Build vocabulary from training data\n",
    "def build_vocab(X_train_full):\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    current_id = 2\n",
    "    for sequence in X_train_full:\n",
    "        for token in (token for sublist in sequence for token in sublist) if isinstance(sequence[0], list) else sequence:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = current_id\n",
    "                current_id += 1\n",
    "    return vocab\n",
    "\n",
    "# Normalize nested sequences during token loading\n",
    "def normalize_sequences(sequences):\n",
    "    return [\n",
    "        [token for sublist in sequence for token in sublist] if sequence and isinstance(sequence[0], list) else sequence or [\"<UNK>\"]\n",
    "        for sequence in sequences\n",
    "    ]\n",
    "\n",
    "# Encode sequences using vocabulary\n",
    "def encode_sequences(sequences, vocab, max_length):\n",
    "    unk_id = vocab[\"<UNK>\"]\n",
    "    encoded = []\n",
    "    for sequence in sequences:\n",
    "        encoded_sequence = [vocab.get(token, unk_id) for token in sequence]\n",
    "        if len(encoded_sequence) > max_length:\n",
    "            encoded_sequence = encoded_sequence[:max_length]\n",
    "        else:\n",
    "            encoded_sequence += [vocab[\"<PAD>\"]] * (max_length - len(encoded_sequence))\n",
    "        encoded.append(encoded_sequence)\n",
    "    return np.array(encoded)\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "X_train_full, y_train_full, X_test, y_test = load_data_for_method(\"basic\")  # Change to method as needed\n",
    "# [\"basic\", \"ast\", \"normalize\", \"syntax\", \"cfg\"]\n",
    "X_train_full = normalize_sequences(X_train_full)\n",
    "X_test = normalize_sequences(X_test)\n",
    "\n",
    "vocab = build_vocab(X_train_full)\n",
    "\n",
    "max_length = 512 # For sequences\n",
    "X_train_encoded = encode_sequences(X_train_full, vocab, max_length)\n",
    "X_test_encoded = encode_sequences(X_test, vocab, max_length)\n",
    "\n",
    "# Step 2: Create DQN environment\n",
    "# Assuming `create_env` returns a gym environment for DQN\n",
    "env = create_env(X_train_encoded, y_train_full)\n",
    "\n",
    "# Step 3: Initialize the DQN model with hyperparameters\n",
    "model = DQN(\n",
    "    \"MlpPolicy\", \n",
    "    env, \n",
    "    verbose=1, \n",
    "    buffer_size=50000, \n",
    "    learning_starts=1000, \n",
    "    batch_size=64, \n",
    "    gamma=0.97, \n",
    "    tau=0.1,\n",
    "    train_freq=4, \n",
    "    target_update_interval=100\n",
    ")\n",
    "\n",
    "# Training Accuracy: 96.46%\n",
    "# Test Accuracy: 52.44%\n",
    "# ROC AUC: 0.52\n",
    "# PRC AUC: 0.65\n",
    "# model = DQN(\n",
    "#     \"MlpPolicy\", \n",
    "#     env, \n",
    "#     verbose=1, \n",
    "#     buffer_size=100000,  # Larger buffer to store more experiences\n",
    "#     learning_starts=5000,  # Delay updates for better initial policy exploration\n",
    "#     batch_size=128,  # Larger batch size for more stable gradients\n",
    "#     gamma=0.995,  # Higher discount factor for longer-term rewards\n",
    "#     tau=0.005,  # Lower tau for smoother target updates\n",
    "#     train_freq=4, \n",
    "#     target_update_interval=100, \n",
    "#     exploration_fraction=0.2,  # Prolonged exploration phase\n",
    "#     exploration_final_eps=0.01,  # Lower final epsilon for better exploitation\n",
    "#     learning_rate=5e-4,  # Slightly increased learning rate\n",
    "#     gradient_steps=-1  # Full training per step for stability\n",
    "# )\n",
    "\n",
    "# Start of notes for many runs of this code\n",
    "# Orginal Results before modification - These are all for the basic data\n",
    "# As shown in directory this report is saved to new_dqn_metrics_output1\n",
    "# Training Accuracy: 91.52%\n",
    "# Test Accuracy: 48.78%\n",
    "\n",
    "# Decrease tau from 0.1 to 0.005 and increase from 100k to 200k time steps got these results:\n",
    "# Training Accuracy: 95.85%\n",
    "# Test Accuracy: 46.34%\n",
    "\n",
    "# Moving tau back from 0.005 and keeping 200k time steps\n",
    "# This is shown in new_dqn_metrics_output2\n",
    "# Training Accuracy: 91.28%\n",
    "# Test Accuracy: 52.44%\n",
    "\n",
    "# Moving from 200k timesteps to 400k timesteps \n",
    "# This is shown in new_dqn_metrics_output3\n",
    "# Training Accuracy: 95.18%\n",
    "# Test Accuracy: 56.10%\n",
    "# ROC AUC: 0.56\n",
    "# PRC AUC: 0.67\n",
    "\n",
    "# Moving from 400k timesteps to 800k timesteps - Overtrained\n",
    "# This is shown in new_dqn_metrics_output4\n",
    "# Training Accuracy: 95.61%\n",
    "# Test Accuracy: 50%\n",
    "# ROC AUC: 0.50\n",
    "# PRC AUC: 0.62\n",
    "\n",
    "# Moving from 800k to 500k timesteps - ROC/PRC curves not saved if file folder not mentioned\n",
    "# Training Accuracy: 94.09%\n",
    "# Test Accuracy: 48.78%\n",
    "\n",
    "# Increase gamma from 0.99 to 0.995\n",
    "# Training Accuracy: 88.96%\n",
    "# Test Accuracy: 47.56%\n",
    "\n",
    "# 0.995 with 400k timesteps\n",
    "# Training Accuracy: 94.70%\n",
    "# Test Accuracy: 51.22%\n",
    "# Run 2:\n",
    "# Training Accuracy: 93.11%\n",
    "# Test Accuracy: 48.78%\n",
    "\n",
    "# Change back to 0.99 with 400k timesteps\n",
    "# Training Accuracy: 94.70%\n",
    "# Test Accuracy: 54.88%\n",
    "\n",
    "# Chagne to 0.98 gamma - BEST so far\n",
    "# Training Accuracy: 95.55%\n",
    "# Test Accuracy: 58.54%\n",
    "\n",
    "# At 0.97 gamma -> Test Accuracy: 46.34%\n",
    "\n",
    "# Up until now it has only been on basic data, with same metrics as \"BEST so far\" for BASIC using on *ast*\n",
    "# This is with changing nothing that got the 58.54% for basic data\n",
    "# ROC AUC: 0.48\n",
    "# PRC AUC: 0.69 - Terrible performance, worse than 50/50 coin toss\n",
    "\n",
    "# Now for *normalize*\n",
    "# Test Accuracy: 39.02%\n",
    "# ROC AUC: 0.39\n",
    "# PRC AUC: 0.57\n",
    "\n",
    "# Now for *syntax*\n",
    "# Test Accuracy: 52.44%\n",
    "# ROC AUC: 0.52\n",
    "# PRC AUC: 0.65 - Slightly better but not great\n",
    "\n",
    "# Now for *cfg*\n",
    "# Currently index out of bounds error\n",
    "\n",
    "# Step 4: Train the DQN model using the training data\n",
    "model.learn(total_timesteps=400000)\n",
    "\n",
    "# Step 5: Save the trained model\n",
    "model.save(\"trained_dqn_model\")\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "train_and_eval_dqn(model, X_train_encoded, y_train_full, X_test_encoded, y_test)\n",
    "\n",
    "# Step 7: Evaluate metrics: ROC curve, PRC curve\n",
    "y_pred_prob = model.predict(X_test_encoded, deterministic=True)[0]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "prc_auc = auc(recall, precision)\n",
    "\n",
    "# Save ROC Curve to file\n",
    "roc_curve_file = \"roc_curve.png\"\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(roc_curve_file)\n",
    "plt.close()\n",
    "\n",
    "# Save Precision-Recall Curve to file\n",
    "prc_curve_file = \"prc_curve.png\"\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, color='blue', lw=2, label=f'PRC curve (area = {prc_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(prc_curve_file)\n",
    "plt.close()\n",
    "\n",
    "# Save metrics report to file\n",
    "metrics_report_file = \"metrics_report.txt\"\n",
    "with open(metrics_report_file, 'w') as f:\n",
    "    f.write(f\"ROC AUC: {roc_auc:.2f}\\n\")\n",
    "    f.write(f\"PRC AUC: {prc_auc:.2f}\\n\")\n",
    "    f.write(f\"\\nROC Curve saved to: {roc_curve_file}\\n\")\n",
    "    f.write(f\"Precision-Recall Curve saved to: {prc_curve_file}\\n\")\n",
    "\n",
    "print(f\"Metrics report saved to {metrics_report_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
